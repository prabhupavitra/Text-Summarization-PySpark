{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4d70fd",
   "metadata": {},
   "source": [
    "### <center>Abstractive Summarization of Earnings Call Transcripts </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9a7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sparknlp in /opt/conda/lib/python3.9/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from sparknlp) (1.20.2)\n",
      "Requirement already satisfied: spark-nlp in /opt/conda/lib/python3.9/site-packages (from sparknlp) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Install sparknlp\n",
    "!pip install --upgrade sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442366b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sparknlp\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c538ca2",
   "metadata": {},
   "source": [
    "> To get started you will need to include the JDBC driver for you particular database on the spark classpath. For example, to connect to postgres from the Spark Shell you would run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5516992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['companyid', 'ticker', 'company_name', 'filepath', 'transcript']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initiating a Spark NLP Session \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.python.worker.memory\",\"6G\") \\\n",
    "    .config(\"spark.driver.memory\",\"24G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.1\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"./mysql-connector-java-8.0.25.jar\")\\\n",
    "    .config(\"spark.worker.cleanup.enabled\",True)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# // Loading data from a JDBC source\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://sem_mysql:3306/test_db\") \\\n",
    "    .option(\"dbtable\", \"earningscall\") \\\n",
    "    .option(\"user\", \"es_user\") \\\n",
    "    .option(\"password\", \"watchword\") \\\n",
    "    .load()\n",
    "\n",
    "print(jdbcDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf31503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------------------+--------+----------+\n",
      "|companyid|ticker|        company_name|filepath|transcript|\n",
      "+---------+------+--------------------+--------+----------+\n",
      "|        1|   AAP|Advance Auto Part...| AAP.txt|          |\n",
      "|        2|    AI|               C3.ai|  AI.txt|          |\n",
      "|        3|  AMAT|Applied Materials...|AMAT.txt|          |\n",
      "|        4|  AMSC|American Supercon...|AMSC.txt|          |\n",
      "|        5|  ANET|Arista Networks, ...|ANET.txt|          |\n",
      "+---------+------+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbcDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e78eec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- local_filepath: string (nullable = true)\n",
      " |-- companyid: integer (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- filepath: string (nullable = true)\n",
      " |-- transcript: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat,lit\n",
    "\n",
    "folder_path = os.getcwd()\n",
    "folder_name =  \"earnings_call_transcripts\"\n",
    "file_path = os.path.join(folder_path, folder_name, '')\n",
    "\n",
    "jdbcDF = jdbcDF.select(concat(lit('file:'),lit(file_path),jdbcDF.filepath) \\\n",
    "               .alias(\"local_filepath\"),'companyid', 'ticker', 'company_name', 'filepath', 'transcript')\n",
    "\n",
    "# jdbcDF.select(concat_ws('/',lit(local_path),lit(folder_name),jdbcDF.filepath)\n",
    "#               .alias(\"local_filepath\"),'companyid', 'ticker', 'company_name', 'filepath', 'transcript').select('local_filepath').toPandas()['local_filepath'][0]\n",
    "\n",
    "jdbcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2a1328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- raw_text: string (nullable = true)\n",
      " |-- index: long (nullable = false)\n",
      " |-- text: string (nullable = true)\n",
      " |-- original_text_length: integer (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "|                path|            raw_text|index|                text|original_text_length|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "|file:/home/jovyan...|��A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 ...|    0|��Advance Auto Pa...|               63600|\n",
      "|file:/home/jovyan...|��C\u00003\u0000.\u0000a\u0000i\u0000 \u0000(\u0000N...|    1|��C3.ai (NYSE:AI)...|               61685|\n",
      "|file:/home/jovyan...|��A\u0000p\u0000p\u0000l\u0000i\u0000e\u0000d\u0000 ...|    2|��Applied Materia...|               27348|\n",
      "|file:/home/jovyan...|��A\u0000m\u0000e\u0000r\u0000i\u0000c\u0000a\u0000n...|    3|��American Superc...|               39742|\n",
      "|file:/home/jovyan...|��A\u0000r\u0000i\u0000s\u0000t\u0000a\u0000 \u0000N...|    4|��Arista Networks...|               32282|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *    \n",
    "# from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "file_path = \"./earnings_call_transcripts/\"\n",
    "texts = spark.sparkContext.wholeTextFiles(file_path,use_unicode=True)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('path', StringType()),\n",
    "    StructField('raw_text', StringType())\n",
    "#     StructField('text_len',IntegerType())\n",
    "])\n",
    "\n",
    "# Add index and Handle escape sequence of \\x00 before tokenizing the document\n",
    "textdf = spark.createDataFrame(texts, schema=schema).persist() \\\n",
    "              .withColumn(\"index\", monotonically_increasing_id()) \\\n",
    "              .withColumn('text', regexp_replace('raw_text', '\\x00|\\n', ''))\n",
    "\n",
    "# Add text length to the table\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "textdf = textdf.withColumn(\"original_text_length\",f.length(col(\"text\")))\n",
    "\n",
    "textdf.printSchema()\n",
    "\n",
    "textdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5adeaff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+--------------------+--------+--------------------+--------------------+\n",
      "|      local_filepath|companyid|ticker|        company_name|filepath|                text|original_text_length|\n",
      "+--------------------+---------+------+--------------------+--------+--------------------+--------------------+\n",
      "|file:/home/jovyan...|        1|   AAP|Advance Auto Part...| AAP.txt|��Advance Auto Pa...|               63600|\n",
      "|file:/home/jovyan...|        2|    AI|               C3.ai|  AI.txt|��C3.ai (NYSE:AI)...|               61685|\n",
      "|file:/home/jovyan...|        3|  AMAT|Applied Materials...|AMAT.txt|��Applied Materia...|               27348|\n",
      "|file:/home/jovyan...|        4|  AMSC|American Supercon...|AMSC.txt|��American Superc...|               39742|\n",
      "|file:/home/jovyan...|        5|  ANET|Arista Networks, ...|ANET.txt|��Arista Networks...|               32282|\n",
      "|file:/home/jovyan...|        6|  AVGO|       Broadcom Ltd |AVGO.txt|��Broadcom Ltd (N...|               36342|\n",
      "|file:/home/jovyan...|        7|   BMA|      Banco Macro SA| BMA.txt|��Banco Macro SA ...|               29441|\n",
      "|file:/home/jovyan...|        8|  CIEN|          CIENA Corp|CIEN.txt|��CIENA Corp (NYS...|               58691|\n",
      "|file:/home/jovyan...|        9|    CM|Canadian Imperial...|  CM.txt|��Canadian Imperi...|               58770|\n",
      "|file:/home/jovyan...|       10|   COO|The Cooper Companies| COO.txt|��The Cooper Comp...|               66913|\n",
      "|file:/home/jovyan...|       11|  COUP|Coupa Software In...|COUP.txt|��Coupa Software ...|               57159|\n",
      "|file:/home/jovyan...|       12|  CRWD|CrowdStrike Holdi...|CRWD.txt|��CrowdStrike Hol...|               55108|\n",
      "|file:/home/jovyan...|       13|   CTK| CooTek (Cayman) Inc| CTK.txt|��CooTek (Cayman)...|               27616|\n",
      "|file:/home/jovyan...|       14|  DADA|  Dada Nexus Limited|DADA.txt|��Dada Nexus Limi...|               46432|\n",
      "|file:/home/jovyan...|       15|   DCI|        Donaldson Co| DCI.txt|��Donaldson Co (N...|               49854|\n",
      "|file:/home/jovyan...|       16|  DLTH|     Duluth Holdings|DLTH.txt|��Duluth Holdings...|               39072|\n",
      "|file:/home/jovyan...|       17|  DOCU|            DocuSign|DOCU.txt|��DocuSign (NASDA...|               55199|\n",
      "|file:/home/jovyan...|       18|  DOOO|             BRP Inc|DOOO.txt|��BRP Inc (NASDAQ...|               52204|\n",
      "|file:/home/jovyan...|       19|  ESTC|         Elastic N V|ESTC.txt|��Elastic N V (NY...|               65614|\n",
      "|file:/home/jovyan...|       20|  EXPR|         Express Inc|EXPR.txt|��Express Inc (NY...|               46635|\n",
      "+--------------------+---------+------+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join tables from mysql with pyspark table\n",
    "jdbcJoinedTable = jdbcDF.join(textdf, jdbcDF.local_filepath == textdf.path, 'left').select(jdbcDF.local_filepath,jdbcDF.companyid,jdbcDF.ticker,jdbcDF.company_name,jdbcDF.filepath, textdf.text,textdf.original_text_length) \n",
    "jdbcJoinedTable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dddf4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36ed54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5_small download started this may take some time.\n",
      "Approximate size to download 139 MB\n",
      "[OK!]\n",
      "root\n",
      " |-- local_filepath: string (nullable = true)\n",
      " |-- companyid: integer (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- filepath: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- original_text_length: integer (nullable = true)\n",
      " |-- doc: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- summaries: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "assembler = DocumentAssembler()\\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('doc')\n",
    "t5 = T5Transformer() \\\n",
    "    .pretrained(\"t5_small\") \\\n",
    "    .setTask(\"summarize:\")\\\n",
    "    .setMaxOutputLength(200)\\\n",
    "    .setInputCols([\"doc\"]) \\\n",
    "    .setOutputCol(\"summaries\")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler, t5])\n",
    "results = pipeline.fit(jdbcJoinedTable).transform(jdbcJoinedTable)\n",
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab67d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applied Materials, Inc. is a company that has a diversified portfolio of products . a lot of people are embracing the technology, and the technology is a key driver . a lot of people are embracing the technology, and the technology is a key driver .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of Earnings call Transcripts for Broadcom Q\n",
    "results.filter(col(\"ticker\") == \"AMAT\").select(['summaries.result']).toPandas()['result'][0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
